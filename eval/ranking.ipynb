{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "models = ['answer_gpt4', 'answer_gpt35', 'answer_bard', 'answer_claude', 'answer_vicuna-13b']\n",
    "reviewers = ['gpt-4', 'claude-1', 'vicuna-13b', 'gpt-3.5-turbo-0301', 'text-bison@001']\n",
    "\n",
    "def review_filename(modelA, modelB, reviewer):\n",
    "    name = f'{modelA}-vs-{modelB}-{reviewer}-reviewer*.jsonl'\n",
    "    glob_path = './ratings-*/' + name\n",
    "    globs = glob.glob(glob_path)\n",
    "    return globs[0]\n",
    "\n",
    "def format_df(df):\n",
    "    # map A wins to -1, B wins to 1, and ties to 0\n",
    "    df.score = df.score.map({1: -1, 2: 1, 3: 0})\n",
    "    df.sort_values(by='question_id', inplace=True)\n",
    "\n",
    "def load_reviews(model, reviewers):\n",
    "    dfs_list = []\n",
    "    for modelA in models:\n",
    "        for modelB in models:\n",
    "            if modelA == modelB:\n",
    "                continue\n",
    "\n",
    "            scores = []\n",
    "            for reviewer in reviewers:\n",
    "                filename = review_filename(modelA, modelB, reviewer)\n",
    "                # get df and add to array\n",
    "                df = pd.read_json(filename, lines=True)[['question_id', 'score']]\n",
    "                format_df(df)\n",
    "                invalid = df.score.isna()\n",
    "                ninvalid = invalid.sum()\n",
    "                if ninvalid > 0:\n",
    "                    print(ninvalid, f'#invalid. {modelA} vs {modelB} by {reviewer}')\n",
    "                    print(df[invalid])\n",
    "                df['model_a'] = modelA\n",
    "                df['model_b'] = modelB\n",
    "                df['reviewer'] = reviewer\n",
    "                dfs_list.append(df)\n",
    "        \n",
    "    # combine all dfs and shuffle\n",
    "    reviews = pd.concat(dfs_list)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_vote(df):\n",
    "    def take_majority(frame):\n",
    "        x = frame.mean()\n",
    "        return np.sign(x)\n",
    "\n",
    "    return df.groupby(['question_id', 'model_a', 'model_b'], as_index=False).agg({'score': take_majority})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# function obtained from https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5\n",
    "def compute_elo(battles, K=32, SCALE=400, BASE=10, INIT_RATING=1000):\n",
    "    rating = defaultdict(lambda: INIT_RATING)\n",
    "\n",
    "    i = 0\n",
    "    for rd, model_a, model_b, winner in battles[['model_a', 'model_b', 'score']].itertuples():\n",
    "        i += 1\n",
    "        ra = rating[model_a]\n",
    "        rb = rating[model_b]\n",
    "        ea = 1 / (1 + BASE ** ((rb - ra) / SCALE))\n",
    "        eb = 1 / (1 + BASE ** ((ra - rb) / SCALE))\n",
    "        if winner == -1:\n",
    "            sa = 1\n",
    "        elif winner == 1:\n",
    "            sa = 0\n",
    "        elif winner == 0:\n",
    "            sa = 0.5\n",
    "        else:\n",
    "            print(\"problem @\", model_a, model_b)\n",
    "            raise Exception(f\"unexpected vote {winner}\")\n",
    "        rating[model_a] += K * (sa - ea)\n",
    "        rating[model_b] += K * (1 - sa - eb)\n",
    "\n",
    "    print(\"Iterations: \", i)\n",
    "    return rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 #invalid. answer_gpt4 vs answer_gpt35 by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "75           76    NaN\n",
      "10 #invalid. answer_gpt4 vs answer_bard by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "11 #invalid. answer_gpt4 vs answer_claude by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "78           79    NaN\n",
      "10 #invalid. answer_gpt4 vs answer_vicuna-13b by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "11 #invalid. answer_gpt35 vs answer_gpt4 by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "76           77    NaN\n",
      "10 #invalid. answer_gpt35 vs answer_bard by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "10 #invalid. answer_gpt35 vs answer_claude by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "10 #invalid. answer_gpt35 vs answer_vicuna-13b by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "12 #invalid. answer_bard vs answer_gpt4 by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "76           77    NaN\n",
      "78           79    NaN\n",
      "10 #invalid. answer_bard vs answer_gpt35 by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "10 #invalid. answer_bard vs answer_claude by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "10 #invalid. answer_bard vs answer_vicuna-13b by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "11 #invalid. answer_claude vs answer_gpt4 by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "76           77    NaN\n",
      "10 #invalid. answer_claude vs answer_gpt35 by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "10 #invalid. answer_claude vs answer_bard by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "10 #invalid. answer_claude vs answer_vicuna-13b by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "12 #invalid. answer_vicuna-13b vs answer_gpt4 by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "76           77    NaN\n",
      "78           79    NaN\n",
      "11 #invalid. answer_vicuna-13b vs answer_gpt35 by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "78           79    NaN\n",
      "11 #invalid. answer_vicuna-13b vs answer_bard by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "78           79    NaN\n",
      "11 #invalid. answer_vicuna-13b vs answer_claude by vicuna-13b\n",
      "    question_id  score\n",
      "60           61    NaN\n",
      "61           62    NaN\n",
      "62           63    NaN\n",
      "63           64    NaN\n",
      "64           65    NaN\n",
      "65           66    NaN\n",
      "66           67    NaN\n",
      "67           68    NaN\n",
      "68           69    NaN\n",
      "69           70    NaN\n",
      "78           79    NaN\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>score</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>reviewer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>answer_gpt4</td>\n",
       "      <td>answer_gpt35</td>\n",
       "      <td>gpt-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>answer_gpt4</td>\n",
       "      <td>answer_gpt35</td>\n",
       "      <td>gpt-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>answer_gpt4</td>\n",
       "      <td>answer_gpt35</td>\n",
       "      <td>gpt-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>answer_gpt4</td>\n",
       "      <td>answer_gpt35</td>\n",
       "      <td>gpt-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>answer_gpt4</td>\n",
       "      <td>answer_gpt35</td>\n",
       "      <td>gpt-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>answer_vicuna-13b</td>\n",
       "      <td>answer_claude</td>\n",
       "      <td>text-bison@001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>77</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>answer_vicuna-13b</td>\n",
       "      <td>answer_claude</td>\n",
       "      <td>text-bison@001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>78</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>answer_vicuna-13b</td>\n",
       "      <td>answer_claude</td>\n",
       "      <td>text-bison@001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>79</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>answer_vicuna-13b</td>\n",
       "      <td>answer_claude</td>\n",
       "      <td>text-bison@001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>80</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>answer_vicuna-13b</td>\n",
       "      <td>answer_claude</td>\n",
       "      <td>text-bison@001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    question_id  score            model_a        model_b        reviewer\n",
       "0             1   -1.0        answer_gpt4   answer_gpt35           gpt-4\n",
       "1             2   -1.0        answer_gpt4   answer_gpt35           gpt-4\n",
       "2             3   -1.0        answer_gpt4   answer_gpt35           gpt-4\n",
       "3             4   -1.0        answer_gpt4   answer_gpt35           gpt-4\n",
       "4             5   -1.0        answer_gpt4   answer_gpt35           gpt-4\n",
       "..          ...    ...                ...            ...             ...\n",
       "75           76   -1.0  answer_vicuna-13b  answer_claude  text-bison@001\n",
       "76           77   -1.0  answer_vicuna-13b  answer_claude  text-bison@001\n",
       "77           78   -1.0  answer_vicuna-13b  answer_claude  text-bison@001\n",
       "78           79   -1.0  answer_vicuna-13b  answer_claude  text-bison@001\n",
       "79           80   -1.0  answer_vicuna-13b  answer_claude  text-bison@001\n",
       "\n",
       "[8000 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = load_reviews(models, reviewers)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:  1600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.compute_elo.<locals>.<lambda>()>,\n",
       "            {'answer_vicuna-13b': 829.7941341216695,\n",
       "             'answer_bard': 768.194934341448,\n",
       "             'answer_claude': 1270.690444483077,\n",
       "             'answer_gpt35': 840.0506986772573,\n",
       "             'answer_gpt4': 1291.2697883765477})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maj = majority_vote(reviews)\n",
    "compute_elo(maj.sample(frac=1, random_state=128))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
