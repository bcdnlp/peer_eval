{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import partial, reduce\n",
    "import statsmodels.stats.inter_rater as ir\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import glob\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Combining Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_map = {\n",
    "    'answer_gpt4': 'gpt-4', \n",
    "    'answer_gpt35': 'gpt-3.5',\n",
    "    'answer_bard': 'bard',\n",
    "    'answer_claude': 'claude', \n",
    "    'answer_vicuna-13b': 'vicuna',\n",
    "}\n",
    "reviewers_map = {\n",
    "    'gpt-4': 'gpt-4',\n",
    "    'gpt-3.5-turbo-0301': 'gpt-3.5',\n",
    "    'text-bison@001': 'bard',\n",
    "    'claude-1': 'claude',\n",
    "    'vicuna-13b': 'vicuna',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_filename(modelA, modelB, reviewer):\n",
    "    name = f'{modelA}-vs-{modelB}-{reviewer}-reviewer*.jsonl'\n",
    "    glob_path = './ratings-*/' + name\n",
    "    globs = glob.glob(glob_path)\n",
    "    return globs[0] if len(globs) > 0 else None\n",
    "\n",
    "def format_df(df):\n",
    "    # map A wins to -1, B wins to 1, and ties to 0\n",
    "    df.score = df.score.map({-1: 0, 1: -1, 2: 1, 3: 0})\n",
    "    df.sort_values(by='question_id', inplace=True)\n",
    "\n",
    "def load_reviews(models_map, reviewers_map):\n",
    "    dfs_list = []\n",
    "    for modelA in models_map.keys():\n",
    "        for modelB in models_map.keys():\n",
    "            if modelA == modelB:\n",
    "                continue\n",
    "\n",
    "            for reviewer in reviewers_map.keys():\n",
    "                filename = review_filename(modelA, modelB, reviewer)\n",
    "                if filename is None:\n",
    "                    print(f'No review file for {modelA} vs {modelB} by {reviewer}')\n",
    "                    continue\n",
    "                # get df and add to array\n",
    "                df = pd.read_json(filename, lines=True)[['question_id', 'score']]\n",
    "                format_df(df)\n",
    "                invalid = df.score.isna()\n",
    "                ninvalid = invalid.sum()\n",
    "                if ninvalid > 0:\n",
    "                    print(ninvalid, f'#invalid. {modelA} vs {modelB} by {reviewer}')\n",
    "                    print(df[invalid])\n",
    "                df['model_a'] = models_map[modelA]\n",
    "                df['model_b'] = models_map[modelB]\n",
    "                df['reviewer'] = reviewers_map[reviewer]\n",
    "                dfs_list.append(df)\n",
    "        \n",
    "    # combine all dfs and shuffle\n",
    "    reviews = pd.concat(dfs_list).sample(frac=1, random_state=42)\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there are multiple reviews for a question from a given review, take the majority vote\n",
    "def human_majority(df):\n",
    "    def take_majority(frame):\n",
    "        x = frame.mean()\n",
    "        return np.sign(x)\n",
    "\n",
    "    return df.groupby(['question_id', 'model_a', 'model_b', 'reviewer'], as_index=False).agg({'score': take_majority})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No review file for answer_gpt4 vs answer_claude by human\n",
      "No review file for answer_gpt35 vs answer_claude by human\n",
      "No review file for answer_bard vs answer_claude by human\n",
      "No review file for answer_vicuna-13b vs answer_claude by human\n"
     ]
    }
   ],
   "source": [
    "auto_reviews = load_reviews(models_map, reviewers_map)\n",
    "human_reviews = human_majority(load_reviews(models_map, {'human': 'human'}))\n",
    "gpt4_reviews = auto_reviews[auto_reviews.reviewer == 'gpt-4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_reviews.index.size\n",
    "human_reviews.index.size\n",
    "gpt4_reviews.index.size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Win Rates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function adapted from https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5\n",
    "def compute_pairwise_win_fraction(battles):\n",
    "    # Times each model wins as Model A\n",
    "    a_win_ptbl = pd.pivot_table(\n",
    "        battles[battles['score'] == -1],\n",
    "        index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0)\n",
    "\n",
    "    # Table counting times each model wins as Model B\n",
    "    b_win_ptbl = pd.pivot_table(\n",
    "        battles[battles['score'] == 1],\n",
    "        index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0)\n",
    "\n",
    "    # Table counting number of A-B pairs\n",
    "    num_battles_ptbl = pd.pivot_table(battles,\n",
    "        index=\"model_a\", columns=\"model_b\", aggfunc=\"size\", fill_value=0)\n",
    "\n",
    "    # Computing the proportion of wins for each model as A and as B\n",
    "    # against all other models\n",
    "    row_beats_col_freq = (\n",
    "        (a_win_ptbl + b_win_ptbl.T) /\n",
    "        (num_battles_ptbl + num_battles_ptbl.T)\n",
    "    )\n",
    "\n",
    "    # Arrange ordering according to proprition of wins\n",
    "    prop_wins = row_beats_col_freq.mean(axis=1).sort_values(ascending=False)\n",
    "    model_names = list(prop_wins.keys())\n",
    "    row_beats_col = row_beats_col_freq.loc[model_names, model_names]\n",
    "    return row_beats_col\n",
    "\n",
    "# function adapted from https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5\n",
    "def visualize_pairwise_win_fraction(battles, title):\n",
    "    row_beats_col = compute_pairwise_win_fraction(battles)\n",
    "    print(row_beats_col)\n",
    "    average_row_beats_col = row_beats_col.mean(axis=1)\n",
    "    print(average_row_beats_col)\n",
    "    average_row_beats_col.name = \"average\"\n",
    "    fig = px.imshow(row_beats_col.merge(average_row_beats_col, on='model_a'), color_continuous_scale='RdBu',\n",
    "                    text_auto=\".2f\", title=title)\n",
    "    fig.update_layout(xaxis_title=\"Model B\",\n",
    "                  yaxis_title=\"Model A\",\n",
    "                  xaxis_side=\"top\",\n",
    "                  title_y=0.07, title_x=0.5)\n",
    "    fig.update_traces(hovertemplate=\n",
    "                  \"Model A: %{y}<br>Model B: %{x}<br>Fraction of A Wins: %{z}<extra></extra>\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_b     gpt-4    claude    vicuna   gpt-3.5      bard\n",
      "model_a                                                  \n",
      "gpt-4         NaN  0.607727  0.824701  0.819178  0.831341\n",
      "claude   0.392273       NaN  0.750337  0.757997  0.794355\n",
      "vicuna   0.175299  0.249663       NaN  0.528571  0.597183\n",
      "gpt-3.5  0.180822  0.242003  0.471429       NaN  0.557471\n",
      "bard     0.168659  0.205645  0.402817  0.442529       NaN\n",
      "model_a\n",
      "gpt-4      0.770737\n",
      "claude     0.673741\n",
      "vicuna     0.387679\n",
      "gpt-3.5    0.362931\n",
      "bard       0.304912\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "Model A: %{y}<br>Model B: %{x}<br>Fraction of A Wins: %{z}<extra></extra>",
         "name": "0",
         "texttemplate": "%{z:.2f}",
         "type": "heatmap",
         "x": [
          "gpt-4",
          "claude",
          "vicuna",
          "gpt-3.5",
          "bard",
          "average"
         ],
         "xaxis": "x",
         "y": [
          "gpt-4",
          "claude",
          "vicuna",
          "gpt-3.5",
          "bard"
         ],
         "yaxis": "y",
         "z": [
          [
           null,
           0.6077265973254086,
           0.8247011952191236,
           0.8191780821917808,
           0.8313413014608234,
           0.7707367940492841
          ],
          [
           0.3922734026745914,
           null,
           0.7503373819163293,
           0.7579972183588317,
           0.7943548387096774,
           0.6737407104148574
          ],
          [
           0.1752988047808765,
           0.2496626180836707,
           null,
           0.5285714285714286,
           0.5971830985915493,
           0.38767898750688123
          ],
          [
           0.18082191780821918,
           0.24200278164116829,
           0.4714285714285714,
           null,
           0.5574712643678161,
           0.36293113381144376
          ],
          [
           0.16865869853917662,
           0.2056451612903226,
           0.4028169014084507,
           0.4425287356321839,
           null,
           0.3049123742175335
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Fraction of Model A Wins for All A vs. B Battles (excluding ties)",
         "x": 0.5,
         "y": 0.07
        },
        "xaxis": {
         "anchor": "y",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "scaleanchor": "y",
         "side": "top",
         "title": {
          "text": "Model B"
         }
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "constrain": "domain",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Model A"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = visualize_pairwise_win_fraction(auto_reviews[auto_reviews.score != 0],\n",
    "      title = \"Fraction of Model A Wins for All A vs. B Battles (excluding ties)\")\n",
    "fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integral Combined Win Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular win fractions\n",
    "def compute_winrates(battles):\n",
    "    num_battles = defaultdict(lambda: 0)\n",
    "    num_wins = defaultdict(lambda: 0)\n",
    "\n",
    "    for i, model_a, model_b, winner in battles[['model_a', 'model_b', 'score']].itertuples():\n",
    "        num_battles[model_a] += 1\n",
    "        num_battles[model_b] += 1\n",
    "        if winner == -1:\n",
    "            num_wins[model_a] += 1\n",
    "        elif winner == 1:\n",
    "            num_wins[model_b] += 1\n",
    "\n",
    "    winrates = {model: num_wins[model] / num_battles[model] for model in num_battles.keys()}\n",
    "    return dict(sorted(winrates.items(), key=lambda item: item[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto Winrates:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpt-4': 0.704375,\n",
       " 'claude': 0.61125,\n",
       " 'vicuna': 0.3471875,\n",
       " 'gpt-3.5': 0.32,\n",
       " 'bard': 0.273125}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Winrates:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpt-4': 0.76875,\n",
       " 'claude': 0.628125,\n",
       " 'vicuna': 0.315625,\n",
       " 'gpt-3.5': 0.253125,\n",
       " 'bard': 0.234375}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4 Winrates:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpt-4': 0.7890625,\n",
       " 'claude': 0.6171875,\n",
       " 'vicuna': 0.284375,\n",
       " 'gpt-3.5': 0.25,\n",
       " 'bard': 0.184375}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Auto Winrates:\")\n",
    "compute_winrates(auto_reviews)\n",
    "print(\"Human Winrates:\")\n",
    "compute_winrates(human_reviews)\n",
    "print(\"GPT4 Winrates:\")\n",
    "compute_winrates(gpt4_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fractional Combined Win Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fractional_winrates(battles):\n",
    "    scores_a = battles.groupby(['model_a']).agg(\n",
    "        sum=pd.NamedAgg(column=\"score\", aggfunc=lambda x: -x.sum()),\n",
    "        count=pd.NamedAgg(column=\"score\", aggfunc=len))\n",
    "\n",
    "    scores_b = battles.groupby(['model_b']).agg(\n",
    "        sum=pd.NamedAgg(column=\"score\", aggfunc='sum'),\n",
    "        count=pd.NamedAgg(column=\"score\", aggfunc=len))\n",
    "        \n",
    "    sum_scores = scores_a.add(scores_b, fill_value=0)\n",
    "    sum_scores.index.name = 'model'\n",
    "    sum_scores['winrate'] = (sum_scores['sum'] / sum_scores['count'] + 1)/2\n",
    "    return sum_scores['winrate'].sort_values(ascending=False).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto Fractional Winrates:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpt-4': 0.74984375,\n",
       " 'claude': 0.66171875,\n",
       " 'vicuna': 0.3934375,\n",
       " 'gpt-3.5': 0.37546875,\n",
       " 'bard': 0.31953125}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Fractional Winrates:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpt-4': 0.821875,\n",
       " 'claude': 0.6890625,\n",
       " 'vicuna': 0.3890625,\n",
       " 'gpt-3.5': 0.3140625,\n",
       " 'bard': 0.2859375}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4 Fractional Winrates:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpt-4': 0.85625,\n",
       " 'claude': 0.70859375,\n",
       " 'vicuna': 0.3484375,\n",
       " 'gpt-3.5': 0.3421875,\n",
       " 'bard': 0.24453124999999998}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Auto Fractional Winrates:\")\n",
    "compute_fractional_winrates(auto_reviews)\n",
    "print(\"Human Fractional Winrates:\")\n",
    "compute_fractional_winrates(human_reviews)\n",
    "print(\"GPT4 Fractional Winrates:\")\n",
    "compute_fractional_winrates(gpt4_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elo Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function adapted from https://colab.research.google.com/drive/1lAQ9cKVErXI1rEYq7hTKNaCQ5Q8TzrI5\n",
    "def compute_elo(battles, K=32, SCALE=400, BASE=10, INIT_RATING=1000):\n",
    "    rating = defaultdict(lambda: INIT_RATING)\n",
    "\n",
    "    for rd, model_a, model_b, score in battles[['model_a', 'model_b', 'score']].itertuples():\n",
    "        ra = rating[model_a]\n",
    "        rb = rating[model_b]\n",
    "        ea = 1 / (1 + BASE ** ((rb - ra) / SCALE))\n",
    "        eb = 1 / (1 + BASE ** ((ra - rb) / SCALE))\n",
    "\n",
    "        # score from scale of [-1, 1] corresponding to [model_a, model_b] wins\n",
    "        sa = (1 - score) / 2\n",
    "\n",
    "        if abs(score) > 1.001:\n",
    "            print(\"problem @\", model_a, model_b)\n",
    "            raise Exception(f\"unexpected vote {score}\")\n",
    "        rating[model_a] += K * (sa - ea)\n",
    "        rating[model_b] += K * (1 - sa - eb)\n",
    "\n",
    "    return dict(sorted(rating.items(), key=lambda x: x[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto ELO:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpt-4': 1153.4482785233479,\n",
       " 'claude': 1129.869490579247,\n",
       " 'vicuna': 928.4186687458919,\n",
       " 'gpt-3.5': 894.7596510899301,\n",
       " 'bard': 893.5039110615811}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human ELO:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpt-4': 1268.5002092438044,\n",
       " 'claude': 1157.0433884486936,\n",
       " 'vicuna': 891.2900929821122,\n",
       " 'gpt-3.5': 868.6004571051353,\n",
       " 'bard': 814.5658522202546}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT4 ELO:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gpt-4': 1250.9361746398897,\n",
       " 'claude': 1136.0313085098653,\n",
       " 'bard': 888.2487114878877,\n",
       " 'vicuna': 867.9868600806446,\n",
       " 'gpt-3.5': 856.7969452817135}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Auto ELO:\")\n",
    "compute_elo(auto_reviews.sample(frac=1, random_state=42))\n",
    "print(\"Human ELO:\")\n",
    "compute_elo(human_reviews.sample(frac=1, random_state=42))\n",
    "print(\"GPT4 ELO:\")\n",
    "compute_elo(gpt4_reviews.sample(frac=1, random_state=42))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm(weights, op, div_factor=pd.DataFrame.sum):\n",
    "    frame = pd.DataFrame(weights.items(), columns=['reviewer', 'weight'])\n",
    "    frame.set_index('reviewer', inplace=True)\n",
    "    weights = frame['weight']\n",
    "    w = op(weights)\n",
    "    # normalize to sum of 1\n",
    "    frame['weight'] = w / div_factor(w)\n",
    "    return frame.to_dict()['weight'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_norm(weights):\n",
    "    def minmax(x):\n",
    "        return (x - x.min()) / (x.max() - x.min())\n",
    "    return get_norm(weights, op=minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_norm(weights):\n",
    "    return get_norm(weights, op=np.exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_norm(weights):\n",
    "    def zscore(weights):\n",
    "        return (weights - weights.mean()) / weights.std()\n",
    "    return get_norm(weights, op=zscore)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Normal Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_normal_test(reviews, iterations, metric):\n",
    "    def print_scores(scores):\n",
    "        df = pd.DataFrame(scores['scores'].items(), columns=['model', 'score'])\n",
    "        df.set_index('model', inplace=True)\n",
    "        print(df)\n",
    "        print()\n",
    "\n",
    "    print(f\"{metric.__name__} for {iterations} iters\")\n",
    "    print_scores(metric(reviews, iterations))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fractional Winrates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_fractional_winrates(battles, weights=None):\n",
    "    if weights is not None:\n",
    "        weights = get_norm(weights, op=lambda x: x, div_factor=pd.DataFrame.mean)\n",
    "    def weighted_sum(frame, negate=False):\n",
    "        if weights is None:\n",
    "            score = frame['score'].sum()\n",
    "        else:\n",
    "            w = frame['reviewer'].map(weights).values\n",
    "            score = (frame['score'] * w).sum()\n",
    "        if negate:\n",
    "            score = -score\n",
    "        count = frame['score'].count()\n",
    "        return pd.Series([score, count], index=['sum', 'count'])\n",
    "        \n",
    "    scores_a = battles.groupby(['model_a']).apply(partial(weighted_sum, negate=True))\n",
    "    scores_b = battles.groupby(['model_b']).apply(weighted_sum)\n",
    "        \n",
    "    sum_scores = scores_a.add(scores_b, fill_value=0)\n",
    "    sum_scores.index.name = 'model'\n",
    "    sum_scores['winrate'] = (sum_scores['sum'] / sum_scores['count'] + 1)/2\n",
    "    return sum_scores['winrate'].sort_values(ascending=False).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_fractional_winrate(reviews, num_iterations, norm_func=minmax_norm):\n",
    "    weights = None\n",
    "    for i in range(num_iterations):\n",
    "        winrates = compute_weighted_fractional_winrates(reviews, weights=weights)\n",
    "        weights = norm_func(winrates)\n",
    "    return { 'scores': winrates, 'weights': weights }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal_fractional_winrate for 5 iters\n",
      "            score\n",
      "model            \n",
      "gpt-4    0.802025\n",
      "claude   0.684978\n",
      "vicuna   0.376249\n",
      "gpt-3.5  0.346164\n",
      "bard     0.290584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_normal_test(auto_reviews, 5, normal_fractional_winrate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elo scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_elo(battles, K=32, SCALE=400, BASE=10, INIT_RATING=1000, weights=None):\n",
    "    if weights is None:\n",
    "        weights = defaultdict(lambda: 1)\n",
    "    else:\n",
    "        weights = get_norm(weights, op=lambda x: x, div_factor=pd.DataFrame.mean)\n",
    "    rating = defaultdict(lambda: INIT_RATING)\n",
    "\n",
    "    for rd, model_a, model_b, reviewer, score in battles[['model_a', 'model_b', 'reviewer', 'score']].itertuples():\n",
    "        w = weights[reviewer]\n",
    "        ra = rating[model_a]\n",
    "        rb = rating[model_b]\n",
    "        ea = 1 / (1 + BASE ** ((rb - ra) / SCALE))\n",
    "        eb = 1 / (1 + BASE ** ((ra - rb) / SCALE))\n",
    "\n",
    "        # score from scale of [-1, 1] corresponding to [model_a, model_b] wins\n",
    "        sa = (1 - score) / 2\n",
    "\n",
    "        if abs(score) > 1.001:\n",
    "            print(\"problem @\", model_a, model_b)\n",
    "            raise Exception(f\"unexpected vote {score}\")\n",
    "        rating[model_a] += w * K * (sa - ea)\n",
    "        rating[model_b] += w * K * (1 - sa - eb)\n",
    "\n",
    "    return dict(sorted(rating.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_elo(reviews, num_iterations, norm_func=minmax_norm):\n",
    "    weights = None\n",
    "    for i in range(num_iterations):\n",
    "        elo = compute_weighted_elo(reviews, weights=weights)\n",
    "        weights = norm_func(elo)\n",
    "    return { 'scores': elo, 'weights': weights }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal_elo for 5 iters\n",
      "               score\n",
      "model               \n",
      "gpt-4    1221.808346\n",
      "vicuna   1015.114640\n",
      "claude    991.452391\n",
      "gpt-3.5   916.227387\n",
      "bard      855.397236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_normal_test(auto_reviews, 5, normal_elo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Majorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_winnertakeall_vote(df, weights=None, EPS=0.1):\n",
    "    def take_weighted_wta_vote(frame):\n",
    "        if weights is None:\n",
    "            w = None\n",
    "        else:\n",
    "            w = frame['reviewer'].map(weights).values\n",
    "        x = np.average(frame['score'], weights=w)\n",
    "        y = np.where(x > EPS, 1, np.where(x < -EPS, -1, 0))\n",
    "        return pd.Series(y, index=['score'])\n",
    "\n",
    "    return df.groupby(['question_id', 'model_a', 'model_b'], as_index=False).apply(take_weighted_wta_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_vote(df, weights=None):\n",
    "    def take_weighted_mean(frame):\n",
    "        if weights is None:\n",
    "            w = None\n",
    "        else:\n",
    "            w = frame['reviewer'].map(weights).values\n",
    "        x = np.average(frame['score'], weights=w)\n",
    "        return pd.Series(x, index=['score'])\n",
    "\n",
    "    return df.groupby(['question_id', 'model_a', 'model_b'], as_index=False).apply(take_weighted_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_majority_test(reviews, iterations, metric):\n",
    "    def print_scores(scores):\n",
    "        df = pd.DataFrame(scores['scores'].items(), columns=['model', 'score'])\n",
    "        df.set_index('model', inplace=True)\n",
    "        print(df)\n",
    "        print()\n",
    "\n",
    "    print(f\"{iterations} iters\\n\")\n",
    "\n",
    "    print(f\"{metric.__name__} w ties and average vote\")\n",
    "    print_scores(metric(reviews, iterations))\n",
    "\n",
    "    print(f\"{metric.__name__} w/o ties and average vote\")\n",
    "    print_scores(metric(reviews, iterations, remove_ties=True))\n",
    "\n",
    "    print(f\"{metric.__name__} w ties and wta vote\")\n",
    "    print_scores(metric(reviews, iterations, voting_func=weighted_winnertakeall_vote))\n",
    "\n",
    "    print(f\"{metric.__name__} w/o ties and wta vote\")\n",
    "    print_scores(metric(reviews, iterations, voting_func=weighted_winnertakeall_vote, remove_ties=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integral Winrates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_winrate(reviews, num_iterations, voting_func=weighted_average_vote, norm_func=minmax_norm, remove_ties=False):\n",
    "    weights = None\n",
    "    for i in range(num_iterations):\n",
    "        maj = voting_func(reviews, weights=weights)\n",
    "        if remove_ties:\n",
    "            maj = maj[maj.score != 0]\n",
    "        winrates = compute_winrates(maj)\n",
    "        weights = norm_func(winrates)\n",
    "    return { 'scores': winrates, 'weights': weights }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 iters\n",
      "\n",
      "majority_winrate w ties and average vote\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            score\n",
      "model            \n",
      "gpt-4    0.382812\n",
      "claude   0.270313\n",
      "vicuna   0.068750\n",
      "gpt-3.5  0.054688\n",
      "bard     0.018750\n",
      "\n",
      "majority_winrate w/o ties and average vote\n",
      "            score\n",
      "model            \n",
      "gpt-4    0.382812\n",
      "claude   0.270313\n",
      "vicuna   0.068750\n",
      "gpt-3.5  0.054688\n",
      "bard     0.018750\n",
      "\n",
      "majority_winrate w ties and wta vote\n",
      "            score\n",
      "model            \n",
      "gpt-4    0.790625\n",
      "claude   0.623437\n",
      "vicuna   0.281250\n",
      "gpt-3.5  0.242188\n",
      "bard     0.173437\n",
      "\n",
      "majority_winrate w/o ties and wta vote\n",
      "            score\n",
      "model            \n",
      "gpt-4    0.898757\n",
      "claude   0.745794\n",
      "vicuna   0.333952\n",
      "gpt-3.5  0.285451\n",
      "bard     0.212644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_majority_test(auto_reviews, 2, majority_winrate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fractional Winrates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_fractional_winrate(reviews, num_iterations, voting_func=weighted_average_vote, norm_func=minmax_norm, remove_ties=False):\n",
    "    weights = None\n",
    "    for i in range(num_iterations):\n",
    "        maj = voting_func(reviews, weights=weights)\n",
    "        if remove_ties:\n",
    "            maj = maj[maj.score != 0]\n",
    "        winrates = compute_fractional_winrates(maj)\n",
    "        weights = norm_func(winrates)\n",
    "    return { 'scores': winrates, 'weights': weights }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal_fractional_winrate for 2 iters\n",
      "            score\n",
      "model            \n",
      "gpt-4    0.801478\n",
      "claude   0.684828\n",
      "vicuna   0.376569\n",
      "gpt-3.5  0.346210\n",
      "bard     0.290916\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for comparison with majority vote using ties and average vote\n",
    "run_normal_test(auto_reviews, 2, normal_fractional_winrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 iters\n",
      "\n",
      "majority_fractional_winrate w ties and average vote\n",
      "            score\n",
      "model            \n",
      "gpt-4    0.801478\n",
      "claude   0.684828\n",
      "vicuna   0.376569\n",
      "gpt-3.5  0.346210\n",
      "bard     0.290916\n",
      "\n",
      "majority_fractional_winrate w/o ties and average vote\n",
      "            score\n",
      "model            \n",
      "gpt-4    0.801272\n",
      "claude   0.684714\n",
      "vicuna   0.376654\n",
      "gpt-3.5  0.346247\n",
      "bard     0.291113\n",
      "\n",
      "majority_fractional_winrate w ties and wta vote\n",
      "            score\n",
      "model            \n",
      "gpt-4    0.850781\n",
      "claude   0.705469\n",
      "vicuna   0.360156\n",
      "gpt-3.5  0.317969\n",
      "bard     0.265625\n",
      "\n",
      "majority_fractional_winrate w/o ties and wta vote\n",
      "            score\n",
      "model            \n",
      "gpt-4    0.898757\n",
      "claude   0.745794\n",
      "vicuna   0.333952\n",
      "gpt-3.5  0.285451\n",
      "bard     0.212644\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_majority_test(auto_reviews, 2, majority_fractional_winrate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elo scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_elo(reviews, num_iterations, voting_func=weighted_average_vote, norm_func=minmax_norm, remove_ties=False):\n",
    "    weights = None\n",
    "    for i in range(num_iterations):\n",
    "        maj = voting_func(reviews, weights=weights)\n",
    "\n",
    "        # randomize order\n",
    "        maj = maj.sample(frac=1, random_state=73)\n",
    "        if remove_ties:\n",
    "            maj = maj[maj.score != 0]\n",
    "        elos = dict(compute_elo(maj))\n",
    "        weights = minmax_norm(elos)\n",
    "    return { 'scores': elos, 'weights': weights }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal_elo for 5 iters\n",
      "               score\n",
      "model               \n",
      "gpt-4    1221.808346\n",
      "vicuna   1015.114640\n",
      "claude    991.452391\n",
      "gpt-3.5   916.227387\n",
      "bard      855.397236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_normal_test(auto_reviews, 5, normal_elo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 iters\n",
      "\n",
      "majority_elo w ties and average vote\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               score\n",
      "model               \n",
      "gpt-4    1202.663763\n",
      "claude   1070.430278\n",
      "vicuna    951.354291\n",
      "bard      901.966066\n",
      "gpt-3.5   873.585602\n",
      "\n",
      "majority_elo w/o ties and average vote\n",
      "               score\n",
      "model               \n",
      "gpt-4    1202.686433\n",
      "claude   1070.417732\n",
      "vicuna    951.356626\n",
      "bard      901.963553\n",
      "gpt-3.5   873.575656\n",
      "\n",
      "majority_elo w ties and wta vote\n",
      "               score\n",
      "model               \n",
      "gpt-4    1320.125321\n",
      "claude   1112.204590\n",
      "vicuna    908.340523\n",
      "bard      846.146818\n",
      "gpt-3.5   813.182749\n",
      "\n",
      "majority_elo w/o ties and wta vote\n",
      "               score\n",
      "model               \n",
      "gpt-4    1331.787077\n",
      "claude   1104.842206\n",
      "vicuna    915.034849\n",
      "bard      834.270473\n",
      "gpt-3.5   814.065395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_majority_test(auto_reviews, 5, majority_elo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_copy = human_reviews.copy()\n",
    "# switch model_a and model_b\n",
    "human_copy['model_a'], human_copy['model_b'] = human_copy['model_b'], human_copy['model_a']\n",
    "human_copy['score'] = -human_copy['score']\n",
    "\n",
    "doubled_human_reviews = pd.concat([human_reviews, human_copy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate(*review_dfs):\n",
    "    for df in review_dfs:\n",
    "        df.sort_values(by=['question_id', 'model_a', 'model_b'], inplace=True)\n",
    "\n",
    "    scores = np.array([df['score'].values for df in review_dfs]).T\n",
    "    print(scores)\n",
    "    subject_category_matrix = ir.aggregate_raters(scores)[0]\n",
    "    print(subject_category_matrix)\n",
    "    return ir.fleiss_kappa(subject_category_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. -0.]\n",
      " [-1. -1.]\n",
      " [ 1.  1.]\n",
      " ...\n",
      " [ 1. -0.]\n",
      " [ 1.  1.]\n",
      " [ 1.  1.]]\n",
      "[[0 1 1]\n",
      " [2 0 0]\n",
      " [0 0 2]\n",
      " ...\n",
      " [0 1 1]\n",
      " [0 0 2]\n",
      " [0 0 2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.39221819366847316"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlate(weighted_winnertakeall_vote(auto_reviews),  doubled_human_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(*review_dfs):\n",
    "    for df in review_dfs:\n",
    "        df.sort_values(by=['question_id', 'model_a', 'model_b'], inplace=True)\n",
    "\n",
    "    scores = np.array([df['score'].values for df in review_dfs])\n",
    "    len_comparisons = scores.shape[1]\n",
    "\n",
    "    comparisons = []\n",
    "    num_comparisons = 0\n",
    "    for i, ratingA in enumerate(scores):\n",
    "        for j, ratingB in enumerate(scores):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            num_comparisons += 1\n",
    "            comparisons.append(np.sum(ratingA == ratingB))\n",
    "    \n",
    "    return reduce(lambda x, y: x + y, comparisons) / (num_comparisons * len_comparisons)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64375"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(weighted_winnertakeall_vote(auto_reviews),  doubled_human_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
